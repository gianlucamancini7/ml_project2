{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.core import display as ICD\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import scipy\n",
    "import os\n",
    "pd.set_option('display.max_columns', 100)\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F      # activation function\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split  # split adata\n",
    "from sklearn.preprocessing import StandardScaler      # Standardzation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_FOLDER='/raid/motus/results/neuralnetwork/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['py_scripts',\n",
       " 'neural_network.ipynb',\n",
       " 'regression_mat_year.csv',\n",
       " 'loss_log',\n",
       " '__pycache__',\n",
       " 'ridge_regression.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'feature_selection_rf.ipynb',\n",
       " 'helpers.py',\n",
       " 'feature_selection_stepwise.ipynb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression matrix manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing regression matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df=pd.read_csv(DATA_FOLDER+'regression_mat_year.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform absolute value and direction in vector components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df=vectorize_wind_speed(tot_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(535470, 23)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tot_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shorten the matrix for developping purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_trial=20000\n",
    "tot_df_small=tot_df.ioc[:number_trial,:]\n",
    "tot_df_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_trial=20000\n",
    "tot_df=tot_df.iloc[:number_trial,:]\n",
    "tot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(tot_df.drop(columns=['u_x', 'u_y','u_z']))\n",
    "Y = np.array(tot_df[['u_x', 'u_y']]) # First consider 1 dimension output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_, X_te_, Y_tr_, Y_te_ = train_test_split(X, Y, test_size = 0.6, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_, x_ev_, y_tr_, y_ev_ = train_test_split(X_tr_, Y_tr_, test_size = 0.5, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ev_hs_, y_ev_hs_=split_hs_test(x_ev_,y_ev_,hs=np.arange(1.5,22,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[28.76798507,  1.5       ,  0.0734543 , ..., 25.666     ,\n",
       "          -1.79353365,  0.26007794],\n",
       "         [21.44765458,  1.5       ,  0.06789737, ..., 18.007     ,\n",
       "           0.22669753, -0.72553478],\n",
       "         [20.72908651,  1.5       , -0.31244171, ..., 21.81066667,\n",
       "           1.92588636,  7.62442933],\n",
       "         ...,\n",
       "         [24.74195018,  1.5       , -0.0818368 , ..., 20.86233333,\n",
       "           1.36180138, -0.5788727 ],\n",
       "         [ 4.77044527,  1.5       , -0.17833659, ...,  1.95466667,\n",
       "           0.71400238, -1.13771844],\n",
       "         [14.44717944,  1.5       , -0.19816249, ..., 22.652     ,\n",
       "          -0.28029994, -0.84167074]]),\n",
       "  array([[ 1.29227797e+01,  5.50000000e+00, -1.48357488e-01, ...,\n",
       "           9.51633333e+00, -4.43550425e+00,  1.79546618e+00],\n",
       "         [ 1.62750196e+01,  5.50000000e+00, -2.33616525e-02, ...,\n",
       "           1.25233333e+01,  5.51567947e-01, -5.21612536e-01],\n",
       "         [ 1.95852663e+01,  5.50000000e+00, -1.30144976e-02, ...,\n",
       "           1.55580000e+01,  1.30770041e+00, -5.52134791e-01],\n",
       "         ...,\n",
       "         [-1.37753320e+00,  5.50000000e+00, -2.71049123e-01, ...,\n",
       "          -1.33700000e+00,  4.31166835e+00, -1.81390342e+00],\n",
       "         [ 2.10800970e+01,  5.50000000e+00, -2.21937751e-02, ...,\n",
       "           2.36243333e+01, -4.84201805e-01, -1.44411250e+00],\n",
       "         [ 1.83399965e+00,  5.50000000e+00, -3.92382177e-01, ...,\n",
       "          -1.83666667e-01,  5.29369574e+00,  5.94697323e+00]]),\n",
       "  array([[ 2.42999481e+01,  9.50000000e+00, -1.18101898e-01, ...,\n",
       "           2.14970000e+01,  2.61757009e+00, -1.71004279e+00],\n",
       "         [ 2.16260475e+01,  9.50000000e+00, -1.78855953e-01, ...,\n",
       "           1.90536667e+01,  1.18469247e+00,  1.42014023e+00],\n",
       "         [-6.48652644e+00,  9.50000000e+00,  5.65064890e-02, ...,\n",
       "          -6.93666667e+00,  4.11071999e+00,  5.32782531e+00],\n",
       "         ...,\n",
       "         [ 1.65584040e+01,  9.50000000e+00,  1.79526903e-02, ...,\n",
       "           1.37713333e+01,  1.63772161e+00, -1.42451931e+00],\n",
       "         [ 1.77875181e+01,  9.50000000e+00, -2.17727803e-01, ...,\n",
       "           2.03136667e+01,  2.10207533e+00, -1.06520905e+00],\n",
       "         [ 3.81020439e+00,  9.50000000e+00,  8.43014264e-02, ...,\n",
       "           5.35366667e+00, -8.65758726e-01, -1.57004507e+00]]),\n",
       "  array([[ 1.74989860e+01,  1.35000000e+01, -5.96585041e-02, ...,\n",
       "           1.94757475e+01, -1.26859260e+00,  9.23129889e-01],\n",
       "         [ 2.23119687e+01,  1.35000000e+01, -8.09538871e-02, ...,\n",
       "           2.44343333e+01, -9.15112435e-01,  3.92395839e-01],\n",
       "         [ 2.59808758e+01,  1.35000000e+01, -1.53187947e-01, ...,\n",
       "           3.20040000e+01, -1.46434442e+00, -1.69425473e+00],\n",
       "         ...,\n",
       "         [ 2.05787490e+01,  1.35000000e+01,  8.16591704e-03, ...,\n",
       "           1.83843333e+01,  9.25510185e-01, -9.02768916e-01],\n",
       "         [ 2.21692788e+01,  1.35000000e+01, -5.96768283e-02, ...,\n",
       "           1.96356667e+01,  2.56549647e+00, -1.49376778e+00],\n",
       "         [ 1.35097618e+01,  1.35000000e+01, -7.26186907e-02, ...,\n",
       "           1.03983333e+01,  2.52963860e+00, -8.14869170e-01]]),\n",
       "  array([[19.74303015, 17.5       , -0.09709312, ..., 16.15933333,\n",
       "          -1.30819443, -4.3062565 ],\n",
       "         [19.26808961, 17.5       , -0.0869127 , ..., 14.991     ,\n",
       "           3.42533078, -1.09914254],\n",
       "         [ 8.37572676, 17.5       , -0.10410847, ...,  6.52333333,\n",
       "           0.90261629, -1.3080156 ],\n",
       "         ...,\n",
       "         [ 8.37179019, 17.5       , -0.02225539, ...,  4.89433333,\n",
       "          -1.76279209,  0.62871921],\n",
       "         [14.41242725, 17.5       ,  0.05979627, ..., 11.49733333,\n",
       "          -0.37452309,  0.97456913],\n",
       "         [11.75496085, 17.5       , -0.08557388, ..., 13.70966667,\n",
       "          -2.39773404,  0.25470475]]),\n",
       "  array([[ 1.72185031e+01,  2.15000000e+01, -1.38052640e-01, ...,\n",
       "           2.35420000e+01, -2.07271958e+00,  4.78106106e-01],\n",
       "         [ 1.17934050e+01,  2.15000000e+01, -2.02032656e-01, ...,\n",
       "           8.88233333e+00, -1.95033401e+00,  3.02483062e+00],\n",
       "         [ 2.80280659e+01,  2.15000000e+01, -1.13039281e-01, ...,\n",
       "           2.53233333e+01, -1.70399808e+00,  9.55937128e-01],\n",
       "         ...,\n",
       "         [ 1.11927991e+01,  2.15000000e+01, -1.19483592e-01, ...,\n",
       "           1.19300000e+01, -1.37341293e+00,  8.09666440e-01],\n",
       "         [ 7.49407709e+00,  2.15000000e+01,  6.05861456e-03, ...,\n",
       "           6.17400000e+00, -3.44456167e+00, -3.20911242e+00],\n",
       "         [ 2.08739330e+01,  2.15000000e+01,  2.43478261e-02, ...,\n",
       "           2.39000000e+01,  2.88025483e-01,  1.00922448e+00]])],\n",
       " [array([[-0.21736448, -0.22779908],\n",
       "         [-0.1469012 , -0.03545162],\n",
       "         [-1.1112289 , -1.30217467],\n",
       "         ...,\n",
       "         [-0.13744361,  0.02759468],\n",
       "         [-0.30119066, -0.1399018 ],\n",
       "         [-0.33814929, -0.28765858]]), array([[-0.42178304, -0.89930919],\n",
       "         [-0.23951742,  0.03507148],\n",
       "         [-0.22256663, -0.02548688],\n",
       "         ...,\n",
       "         [-0.9523531 , -0.06707559],\n",
       "         [-0.13236838, -0.61760872],\n",
       "         [-0.51541544, -1.58856071]]), array([[-0.66263732, -0.52134013],\n",
       "         [-0.52114082,  0.43068431],\n",
       "         [-1.72410227,  1.19411799],\n",
       "         ...,\n",
       "         [-0.3577921 , -0.20408186],\n",
       "         [-0.61720702,  0.05677962],\n",
       "         [-0.23868009, -0.97263993]]), array([[-1.0919279 ,  0.49531893],\n",
       "         [-0.85597315,  0.08701647],\n",
       "         [-0.61750751, -1.23019793],\n",
       "         ...,\n",
       "         [-0.5568582 , -1.01584652],\n",
       "         [-1.71105664, -0.30960977],\n",
       "         [ 0.19590804, -1.89139035]]), array([[-2.22862341, -3.04245824],\n",
       "         [ 2.75027243, -1.43978928],\n",
       "         [ 0.60938734, -1.09144687],\n",
       "         ...,\n",
       "         [-0.79941206,  0.99662319],\n",
       "         [-0.30014418,  0.62470971],\n",
       "         [-2.13352078,  0.08593398]]), array([[-2.07704793,  0.10218307],\n",
       "         [ 1.67574801,  2.62057614],\n",
       "         [-1.71330988,  0.56771986],\n",
       "         ...,\n",
       "         [-1.33000792,  0.56772528],\n",
       "         [-2.37861435, -3.49705738],\n",
       "         [ 0.57252106,  0.92114035]])])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_hs_test(X_te_,Y_te_,hs=np.arange(1.5,22,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spring, df_summer, df_autumn, df_winter=season_splitter(tot_df)\n",
    "df_seasons=[df_spring, df_summer, df_autumn, df_winter]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data for season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasons_net(df_seasons):\n",
    "    \n",
    "    for df_season in df_seasons:\n",
    "        \n",
    "        X = np.array(df_season.drop(columns=['u_x', 'u_y','u_z']))\n",
    "        Y = np.array(df_season[['u_x', 'u_y']])\n",
    "        \n",
    "        #creating the train set and the test set\n",
    "        X_season_TR_, X_season_te_, Y_TR_, Y_te_ = train_test_split(X, Y, test_size = 0.6, random_state = 0)\n",
    "\n",
    "        #creating the train set and the evaluation test out of the train set\n",
    "        x_season_tr_, x_season_ev_, y_season_tr_, y_season_ev_ = train_test_split(X_season_TR_, Y_TR_, test_size = 0.5, random_state = 0)\n",
    "\n",
    "        #splitting the evaluation data in anemometers \n",
    "        x_ev_hs_, y_ev_hs_=split_hs_test(x_season_ev_,y_season_ev_,hs=np.arange(1.5,22,4))\n",
    "        \n",
    "        k = StandardScaler().fit(x_season_tr_)\n",
    "        x_season_tr_ = k.transform(x_season_tr_)\n",
    "        \n",
    "        # standardize the evaluation test according to the metric of the training set\n",
    "        \n",
    "        x_season_ev_ = k.transform(x_season_ev_)\n",
    "        \n",
    "        # standardize the anemometers values of the evaluation test according to the metric of the training set\n",
    "        x_ev_standard_hs_=[]\n",
    "        for x_ev_h_ in x_ev_hs_:\n",
    "    \n",
    "            x_ev_standard_hs_.append(k.transform(x_ev_h_))\n",
    "        \n",
    "        # converting the evalation and training data into tensors and into variables\n",
    "        elements_array=[x_season_tr_, x_season_ev_, y_season_tr_, y_season_ev_]\n",
    "        elements_variables=[]\n",
    "        \n",
    "        for element in elements_array:\n",
    "            \n",
    "            elements_variables.append(Variable(torch.from_numpy(element).type(torch.FloatTensor)))\n",
    "        \n",
    "        # converting the anemometers arrays into tensors and into variables\n",
    "        x_ev_standard_hs_variable=[]\n",
    "        y_ev_hs_variable=[]\n",
    "\n",
    "        for x_ev_standard_h_, y_ev_h_ in zip(x_ev_standard_hs_, y_ev_hs_):\n",
    "\n",
    "            x_ev_standard_hs_variable.append(Variable(torch.from_numpy(x_ev_standard_h_).type(torch.FloatTensor)))\n",
    "            y_ev_hs_variable.append(Variable(torch.from_numpy(y_ev_h_).type(torch.FloatTensor)))\n",
    "            \n",
    "        return elements_variables, x_ev_standard_hs_variable, y_ev_hs_variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c=seasons_net(df_seasons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the test data for performance evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(x_tr_, X_te_, Y_te_):\n",
    "    \n",
    "    #standardize test data\n",
    "    k = StandardScaler().fit(x_tr_)\n",
    "    \n",
    "    #splitting the test data in anemometers \n",
    "    X_te_hs_, Y_te_hs_=split_hs_test(X_te_,Y_te_,hs=np.arange(1.5,22,4))\n",
    "    \n",
    "    X_te_ = k.transform(X_te_)\n",
    "    \n",
    "    # standardize the anemometers values of the test set according to the metric of the training set\n",
    "    X_te_standard_hs_=[]\n",
    "    for X_te_h_ in X_te_hs_:\n",
    "\n",
    "        X_te_standard_hs_.append(k.transform(X_te_h_))\n",
    "\n",
    "    # converting the testing data into tensors and into variables\n",
    "    elements_array=[X_te_, Y_te_]\n",
    "    elements_variables=[]\n",
    "\n",
    "    for element in elements_array:\n",
    "\n",
    "        elements_variables.append(Variable(torch.from_numpy(element).type(torch.FloatTensor)))\n",
    "    \n",
    "    # converting the anemometers arrays into testing tensors and into variables\n",
    "    X_te_standard_hs_variable=[]\n",
    "    Y_te_hs_variable=[]\n",
    "\n",
    "    for X_te_standard_h_, Y_te_h_ in zip(X_te_standard_hs_, Y_te_hs_):\n",
    "\n",
    "        X_te_standard_hs_variable.append(Variable(torch.from_numpy(X_te_standard_h_).type(torch.FloatTensor)))\n",
    "        Y_te_hs_variable.append(Variable(torch.from_numpy(Y_te_h_).type(torch.FloatTensor)))\n",
    "    \n",
    "    return elements_variables, X_te_standard_hs_variable, Y_te_hs_variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c=prepare_test_data(x_tr_, X_te_, Y_te_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the training data and tensor transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(x_tr_,x_ev_,y_tr_,y_ev_):\n",
    "    \n",
    "    k = StandardScaler().fit(x_tr_)\n",
    "    x_tr_ = k.transform(x_tr_)\n",
    "    x_ev_ = k.transform(x_ev_)\n",
    "\n",
    "    x_tr = Variable(torch.from_numpy(x_tr_).type(torch.FloatTensor))\n",
    "    y_tr = Variable(torch.from_numpy(y_tr_).type(torch.FloatTensor))\n",
    "    x_ev = Variable(torch.from_numpy(x_ev_).type(torch.FloatTensor))\n",
    "    y_ev = Variable(torch.from_numpy(y_ev_).type(torch.FloatTensor))\n",
    "\n",
    "    x_ev_standard_hs_=[]\n",
    "    for x_ev_h_ in x_ev_hs_:\n",
    "\n",
    "        x_ev_standard_hs_.append(k.transform(x_ev_h_))\n",
    "\n",
    "    x_ev_standard_hs=[]\n",
    "\n",
    "    for x_ev_standard_h_ in x_ev_standard_hs_:\n",
    "\n",
    "        x_ev_standard_hs.append(torch.from_numpy(x_ev_standard_h_).type(torch.FloatTensor))\n",
    "\n",
    "    x_ev_standard_hs_variable=[]\n",
    "    y_ev_hs_variable=[]\n",
    "\n",
    "    for x_ev_standard_h, y_ev_h_ in zip(x_ev_standard_hs, y_ev_hs_):\n",
    "\n",
    "        x_ev_standard_hs_variable.append(Variable(x_ev_standard_h))\n",
    "        y_ev_hs_variable.append(Variable(torch.from_numpy(y_ev_h_).type(torch.FloatTensor)))\n",
    "        \n",
    "    return x_tr,y_tr,x_ev,y_ev,x_ev_standard_hs_variable,y_ev_hs_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (hidden1): Linear(in_features=20, out_features=100, bias=True)\n",
      "  (hidden2): Linear(in_features=100, out_features=100, bias=True)\n",
      "  (predict): Linear(in_features=100, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, n_feature, n_hidden, n_output):\n",
    "        super(Net, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(n_feature, n_hidden)  \n",
    "        self.hidden2 = torch.nn.Linear(n_hidden, n_hidden)\n",
    "        self.predict = torch.nn.Linear(n_hidden, n_output)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        a1 = F.relu(self.hidden1(x)) \n",
    "        a2 = F.relu(self.hidden2(a1))\n",
    "        #a1 = torch.sigmoid(self.hidden(x))\n",
    "        #a1 = self.hidden(x) # 隐藏层用 relu\n",
    "        y = self.predict(a2)\n",
    "        return y\n",
    "\n",
    "net = Net(n_feature=x_tr_.shape[1], n_hidden=100, n_output=2)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.MSELoss()   \n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the test dataset according to the different anemometers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the vectors in tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_net(x_tr,y_tr,x_ev, y_ev,epochs):\n",
    "    # the inputs of the function are variables tensors and epoch is the \n",
    "    # number of iterations of the backward propagation\n",
    "\n",
    "    losses=[]\n",
    "    mse_0=1000\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        prediction = net(x_tr)\n",
    "        loss = loss_func(prediction, y_tr)\n",
    "        \n",
    "        mse=np.average((y_ev-prediction)^2)\n",
    "        if mse_0-mse<0:\n",
    "            break\n",
    "        mse_0=mse\n",
    "\n",
    "        #print loss\n",
    "        optimizer.zero_grad()    # clear gradients for next train\n",
    "        loss.backward()          # backpropagation\n",
    "        optimizer.step()         # update（w、b）\n",
    "        \n",
    "        mse_s=[]\n",
    "        if t%10==0:\n",
    "            print ('The loss is',loss.detach().numpy())\n",
    "            print ('The mse is',mse)\n",
    "            \n",
    "            losses.append(loss.detach().numpy())\n",
    "            mse_s.append(mse)\n",
    "\n",
    "    return pd.Series(losses).to_csv(RESULTS_FOLDER+'loss_log'), pd.Series(mses).to_csv(RESULTS_FOLDER+'mses_log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction visualisation per anemometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds=[]\n",
    "for i in x_ev_standard_hs_variable:\n",
    "    net.eval()\n",
    "    predict = net(i)\n",
    "    y_pred = predict.data.numpy()\n",
    "    y_preds.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_ys(y_preds,y_ev_,RESULTS_FOLDER,save=False,interval=[100,200],name='graph')\n",
    "#inputs of these functions are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (y_pred, y_ev_i) in enumerate(zip(y_preds, y_ev_)):\n",
    "    \n",
    "    print ('MSE anemometer'+' ', i , 'is', np.mean(np.square(y_pred-y_ev_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_trial_standard_hs=[]\n",
    "\n",
    "for x_te_trial_standard_h_ in x_te_trial_standard_hs_:\n",
    "    \n",
    "    x_te_trial_standard_hs.append(torch.from_numpy(x_te_trial_standard_h_).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_trial_ = torch.from_numpy(x_tr_trial_)\n",
    "y_tr_trial_ = torch.from_numpy(y_tr_trial_)\n",
    "x_te_trial_ = torch.from_numpy(x_te_trial_)\n",
    "y_te_trial_ = torch.from_numpy(y_te_trial_)\n",
    "x_tr_trial = x_tr_trial_.type(torch.FloatTensor)\n",
    "y_tr_trial = y_tr_trial_.type(torch.FloatTensor)\n",
    "x_te_trial = x_te_trial_.type(torch.FloatTensor)\n",
    "y_te_trial = y_te_trial_.type(torch.FloatTensor)\n",
    "x_tr_trial, y_tr_trial = Variable(x_tr_trial), Variable(y_tr_trial)\n",
    "x_te_trial, y_te_trial = Variable(x_te_trial), Variable(y_te_trial)\n",
    "\n",
    "x_te_trial_standard_hs_variable=[]\n",
    "y_te_trial_hs_variable=[]\n",
    "\n",
    "for x_te_trial_standard_h, y_te_trial_s_ in zip(x_te_trial_standard_hs, y_te_trial_hs_):\n",
    "    \n",
    "    x_te_trial_standard_hs_variable.append(Variable(x_te_trial_standard_h ))\n",
    "    y_te_trial_hs_variable.append(Variable(torch.from_numpy(y_te_trial_s_).type(torch.FloatTensor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the Data for Season "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spring_tr_standard=(df_spring_tr.drop(columns=['u_x', 'u_y','u_z'])-df_spring_tr.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_spring_tr.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_autumn_tr_standard=(df_autumn_tr.drop(columns=['u_x', 'u_y','u_z'])-df_autumn_tr.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_autumn_tr.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_summer_tr_standard=(df_summer_tr.drop(columns=['u_x', 'u_y','u_z'])-df_summer_tr.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_summer_tr.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_winter_tr_standard=(df_winter_tr.drop(columns=['u_x', 'u_y','u_z'])-df_winter_tr.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_winter_tr.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spring_tr=np.array(df_spring_tr[['u_x','u_y']])\n",
    "y_summer_tr=np.array(df_summer_tr[['u_x','u_y']])\n",
    "y_autumn_tr=np.array(df_autumn_tr[['u_x','u_y']])\n",
    "y_winter_tr=np.array(df_winter_tr[['u_x','u_y']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_spring_te_standard=(df_spring_te.drop(columns=['u_x', 'u_y','u_z'])-df_spring_te.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_spring_te.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_autumn_te_standard=(df_autumn_te.drop(columns=['u_x', 'u_y','u_z'])-df_autumn_te.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_autumn_te.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_summer_te_standard=(df_summer_te.drop(columns=['u_x', 'u_y','u_z'])-df_summer_te.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_summer_te.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)\n",
    "X_winter_te_standard=(df_winter_te.drop(columns=['u_x', 'u_y','u_z'])-df_winter_te.drop(columns=['u_x', 'u_y','u_z']).mean(axis=0))/df_winter_te.drop(columns=['u_x', 'u_y','u_z']).std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_spring_te=np.array(df_spring_te[['u_x','u_y']])\n",
    "y_summer_te=np.array(df_summer_te[['u_x','u_y']])\n",
    "y_autumn_te=np.array(df_autumn_te[['u_x','u_y']])\n",
    "y_winter_te=np.array(df_winter_te[['u_x','u_y']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing the training data (trial set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = StandardScaler().fit(x_tr_trial_)\n",
    "x_tr_trial_ = k.transform(x_tr_trial_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing anemometer test (trial set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_trial_standard_hs_=[]\n",
    "for x_te_trial_h_ in x_te_trial_hs_:\n",
    "    \n",
    "    x_te_trial_standard_hs_.append(k.transform(x_te_trial_h_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data (trial set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trial = np.array(tot_df_small.drop(columns=['u_x', 'u_y','u_z']))\n",
    "Y_trial = np.array(tot_df_small[['u_x', 'u_y']]) # First consider 1 dimension output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr_trial_, x_te_trial_, y_tr_trial_, y_te_trial_ = train_test_split(X_trial, Y_trial, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_te_trial_hs_, y_te_trial_hs_=split_hs_test(x_te_trial_,y_te_trial_,hs=np.arange(1.5,22,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
